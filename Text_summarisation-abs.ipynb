{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headlines', 'text'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98401"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "\n",
    "#reading the data set - news data sets\n",
    "\n",
    "data_set = pd.read_csv(\"news_summary.csv\")\n",
    "print(data_set.columns)\n",
    "data_set\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4995</td>\n",
       "      <td>25 fake call centres busted since July in Noid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4996</td>\n",
       "      <td>Govt to meet fiscal deficit target despite GST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4997</td>\n",
       "      <td>Ikea to invest â¹5,000 cr, create 8,000 jobs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4998</td>\n",
       "      <td>GST on under-construction flats, houses may be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4999</td>\n",
       "      <td>Govt bank customers paid â¹10,000 crore in fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              headlines\n",
       "0     upGrad learner switches to career in ML & Al w...\n",
       "1     Delhi techie wins free food from Swiggy for on...\n",
       "2     New Zealand end Rohit Sharma-led India's 12-ma...\n",
       "3     Aegon life iTerm insurance plan helps customer...\n",
       "4     Have known Hirani for yrs, what if MeToo claim...\n",
       "...                                                 ...\n",
       "4995  25 fake call centres busted since July in Noid...\n",
       "4996  Govt to meet fiscal deficit target despite GST...\n",
       "4997  Ikea to invest â¹5,000 cr, create 8,000 jobs ...\n",
       "4998  GST on under-construction flats, houses may be...\n",
       "4999  Govt bank customers paid â¹10,000 crore in fi...\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividing Dataset into X and Y\n",
    "text_review = data_set.drop(columns = 'headlines' , axis = 1)\n",
    "text_review = text_review.iloc[:5000,:]\n",
    "head_lines = data_set.drop(columns = 'text', axis = 1)\n",
    "head_lines = head_lines.iloc[:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "#punctuations removal\n",
    "\n",
    "def cleaning(matter):\n",
    "    temp = []\n",
    "    for content in matter:\n",
    "        cleaned_text = re.sub(r'\\[[0-9]*\\]',' ',content)\n",
    "        final_text=re.sub(r'\\s+',' ',cleaned_text)\n",
    "        final_text=re.sub(r'\\[[a-zA-Z]*\\]',' ',final_text)\n",
    "        final_text=re.sub(r'\\s+',' ',final_text)\n",
    "        final_text=re.sub(r'\\(([^)]*)\\)',' ',final_text)\n",
    "        final_text=re.sub(r'\\s+',' ',final_text)\n",
    "        temp.append(final_text)\n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the text\n",
    "p_text = cleaning(text_review.text)\n",
    "p_headlines = cleaning(head_lines.headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contractions\n",
    "#https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#stop word removal and contractions replacing\n",
    "sw=nltk.corpus.stopwords.words(\"english\")\n",
    "text_final = []\n",
    "text_headlines = []\n",
    "for text_1 in p_text:\n",
    "    temp = text_1.split()\n",
    "    new_text = []\n",
    "    for word in temp:\n",
    "        word = word.lower()\n",
    "        if word in contractions.keys():\n",
    "            new_text.append(contractions[word])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    t = \" \".join(new_text)\n",
    "    text_final.append(t)\n",
    "for text_2 in p_headlines:\n",
    "    temp = text_2.split()\n",
    "    new_text1 = []\n",
    "    for word in temp:\n",
    "        word = word.lower()\n",
    "        if word in contractions.keys():\n",
    "            new_text1.append(contractions[word])\n",
    "        else:\n",
    "            new_text1.append(word)\n",
    "    t1 = \" \".join(new_text1)\n",
    "    text_headlines.append(t1)\n",
    "print(type(text_final))\n",
    "print(type(text_headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25407\n"
     ]
    }
   ],
   "source": [
    "#counting the number of words and sentences in the text\n",
    "word_count = {}\n",
    "for sentence in text_final:\n",
    "  for word in nltk.word_tokenize(sentence):\n",
    "    if word in word_count.keys():\n",
    "      word_count[word] += 1\n",
    "    else:\n",
    "      word_count[word] = 1\n",
    "\n",
    "for sentence in text_headlines:\n",
    "  for word in nltk.word_tokenize(sentence):\n",
    "    if word in word_count.keys():\n",
    "      word_count[word]+=1\n",
    "    else:\n",
    "      word_count[word] = 1\n",
    "print(len(word_count))#total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#introducing the word embeddings for the data\n",
    "#preparing the data for word_vectorization\n",
    "temp1 = [nltk.word_tokenize(sentence) for sentence in text_final]\n",
    "temp2 = [nltk.word_tokenize(sentence) for sentence in text_headlines]\n",
    "total_text = temp1+temp2\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# generating the word embeddings using the input text\n",
    "#considering the words in the input text that appear atleast 5 times\n",
    "#default size(dimension) of Word2Vec are 100\n",
    "\n",
    "word_embeddings = Word2Vec(total_text, min_count = 1)\n",
    "word_embeddings_vocab = embeddings_model.wv.vocab\n",
    "type(word_embeddings) #word embeddings from input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869\n",
      "1873\n"
     ]
    }
   ],
   "source": [
    "#converting words to int \n",
    "#limiting the vocab that is allowed to appear\n",
    "threshold = 25  #words repeating atleast 25 times\n",
    "\n",
    "vocab_to_int = {}\n",
    "value = 0\n",
    "for word,count in word_count.items():\n",
    "    if count >= threshold:\n",
    "        vocab_to_int[word] = value\n",
    "        value +=1 \n",
    "print(len(vocab_to_int))\n",
    "\n",
    "# Special tokens assigined to the vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "    \n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "\n",
    "print(len(int_to_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873\n",
      "[[-1.2565484  -0.04227275  0.05313071 ... -0.3204583   1.342404\n",
      "   0.55143905]\n",
      " [-0.953348   -1.5597372   0.5479414  ... -0.22231664  1.2996067\n",
      "   0.09413171]\n",
      " [-0.8027061  -0.94861317  1.1931536  ... -0.5199362   0.8372211\n",
      "   0.18912937]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohankalyanveeraghanta/Software-Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/mohankalyanveeraghanta/Software-Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_dim = 100\n",
    "words_size = len(vocab_to_int)\n",
    "\n",
    "# Creating an embedding matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((words_size,embedding_dim),dtype=np.float32)\n",
    "\n",
    "for word,i in vocab_to_int.items():\n",
    "    if word in word_embeddings:\n",
    "        word_embedding_matrix[i] = word_embeddings[word]\n",
    "\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))\n",
    "print(word_embedding_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
